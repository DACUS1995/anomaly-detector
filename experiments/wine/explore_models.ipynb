{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"D:\\\\Storage\\\\AnomalyDetection\\\\wine\\\\benchmarks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of files: 1210\nExample path: D:\\Storage\\AnomalyDetection\\wine\\benchmarks\\wine_benchmark_0001.csv\n"
    }
   ],
   "source": [
    "file_list = list(pathlib.Path(dataset_path).glob('*.csv'))\n",
    "print(f\"Number of files: {len(file_list)}\")\n",
    "print(f\"Example path: {file_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(185150, 50)"
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       original.label     diff.score  fixed.acidity  volatile.acidity  \\\ncount   185150.000000  185150.000000  185150.000000     185150.000000   \nmean         5.821820       0.184060      -0.003067         -0.001873   \nstd          0.873348       0.179302       0.994429          0.994004   \nmin          3.000000       0.002164      -2.634386         -1.577208   \n25%          5.000000       0.061595      -0.628884         -0.666110   \n50%          6.000000       0.127601      -0.166076         -0.301671   \n75%          6.000000       0.241031       0.373866          0.366468   \nmax          9.000000       0.996894       6.698910          7.533774   \n\n         citric.acid  residual.sugar      chlorides  free.sulfur.dioxide  \\\ncount  185150.000000   185150.000000  185150.000000        185150.000000   \nmean       -0.004124        0.000977      -0.005601             0.000519   \nstd         0.998160        0.999860       0.975796             0.994600   \nmin        -2.192664       -1.017956      -1.342536            -1.663455   \n25%        -0.472297       -0.765739      -0.514759            -0.762016   \n50%        -0.059409       -0.513522      -0.257863            -0.085936   \n75%         0.491108        0.558401       0.255930             0.590143   \nmax         9.230570       12.685846      15.840967            14.562446   \n\n       total.sulfur.dioxide        density  ...     noise..24     noise..25  \\\ncount         185150.000000  185150.000000  ...  37030.000000  37030.000000   \nmean              -0.000257      -0.002645  ...     -0.002497     -0.001473   \nstd                0.998973       1.001061  ...      1.010434      0.997907   \nmin               -1.941631      -2.529997  ...     -2.975988     -2.789406   \n25%               -0.667787      -0.792562  ...     -0.702685     -0.649336   \n50%                0.039904       0.061149  ...     -0.177258     -0.200775   \n75%                0.712210       0.758124  ...      0.528136      0.477463   \nmax                5.736815      14.767654  ...     15.840967     15.840967   \n\n          noise..26     noise..27     noise..28     noise..29     noise..30  \\\ncount  37030.000000  37030.000000  37030.000000  37030.000000  37030.000000   \nmean       0.004486     -0.002218      0.004315     -0.006058     -0.006279   \nstd        0.998337      1.000018      1.016338      0.992243      1.007432   \nmin       -2.433288     -2.634386     -2.557251     -3.100376     -2.975988   \n25%       -0.680540     -0.628884     -0.706019     -0.737004     -0.674810   \n50%       -0.119451     -0.166076     -0.166076     -0.177258     -0.115064   \n75%        0.593772      0.451001      0.529090      0.600438      0.533803   \nmax       12.685846      9.230570     15.840967     14.767654      9.870119   \n\n          noise..31     noise..32     noise..33  \ncount  37030.000000  37030.000000  37030.000000  \nmean      -0.004379      0.000239     -0.009774  \nstd        1.003568      1.003339      0.993068  \nmin       -3.100376     -3.100376     -3.100376  \n25%       -0.706019     -0.609926     -0.680540  \n50%       -0.166076     -0.128224     -0.142276  \n75%        0.528136      0.559923      0.516365  \nmax        9.870119     15.840967     14.767654  \n\n[8 rows x 46 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original.label</th>\n      <th>diff.score</th>\n      <th>fixed.acidity</th>\n      <th>volatile.acidity</th>\n      <th>citric.acid</th>\n      <th>residual.sugar</th>\n      <th>chlorides</th>\n      <th>free.sulfur.dioxide</th>\n      <th>total.sulfur.dioxide</th>\n      <th>density</th>\n      <th>...</th>\n      <th>noise..24</th>\n      <th>noise..25</th>\n      <th>noise..26</th>\n      <th>noise..27</th>\n      <th>noise..28</th>\n      <th>noise..29</th>\n      <th>noise..30</th>\n      <th>noise..31</th>\n      <th>noise..32</th>\n      <th>noise..33</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>...</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.821820</td>\n      <td>0.184060</td>\n      <td>-0.003067</td>\n      <td>-0.001873</td>\n      <td>-0.004124</td>\n      <td>0.000977</td>\n      <td>-0.005601</td>\n      <td>0.000519</td>\n      <td>-0.000257</td>\n      <td>-0.002645</td>\n      <td>...</td>\n      <td>-0.002497</td>\n      <td>-0.001473</td>\n      <td>0.004486</td>\n      <td>-0.002218</td>\n      <td>0.004315</td>\n      <td>-0.006058</td>\n      <td>-0.006279</td>\n      <td>-0.004379</td>\n      <td>0.000239</td>\n      <td>-0.009774</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.873348</td>\n      <td>0.179302</td>\n      <td>0.994429</td>\n      <td>0.994004</td>\n      <td>0.998160</td>\n      <td>0.999860</td>\n      <td>0.975796</td>\n      <td>0.994600</td>\n      <td>0.998973</td>\n      <td>1.001061</td>\n      <td>...</td>\n      <td>1.010434</td>\n      <td>0.997907</td>\n      <td>0.998337</td>\n      <td>1.000018</td>\n      <td>1.016338</td>\n      <td>0.992243</td>\n      <td>1.007432</td>\n      <td>1.003568</td>\n      <td>1.003339</td>\n      <td>0.993068</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.000000</td>\n      <td>0.002164</td>\n      <td>-2.634386</td>\n      <td>-1.577208</td>\n      <td>-2.192664</td>\n      <td>-1.017956</td>\n      <td>-1.342536</td>\n      <td>-1.663455</td>\n      <td>-1.941631</td>\n      <td>-2.529997</td>\n      <td>...</td>\n      <td>-2.975988</td>\n      <td>-2.789406</td>\n      <td>-2.433288</td>\n      <td>-2.634386</td>\n      <td>-2.557251</td>\n      <td>-3.100376</td>\n      <td>-2.975988</td>\n      <td>-3.100376</td>\n      <td>-3.100376</td>\n      <td>-3.100376</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.000000</td>\n      <td>0.061595</td>\n      <td>-0.628884</td>\n      <td>-0.666110</td>\n      <td>-0.472297</td>\n      <td>-0.765739</td>\n      <td>-0.514759</td>\n      <td>-0.762016</td>\n      <td>-0.667787</td>\n      <td>-0.792562</td>\n      <td>...</td>\n      <td>-0.702685</td>\n      <td>-0.649336</td>\n      <td>-0.680540</td>\n      <td>-0.628884</td>\n      <td>-0.706019</td>\n      <td>-0.737004</td>\n      <td>-0.674810</td>\n      <td>-0.706019</td>\n      <td>-0.609926</td>\n      <td>-0.680540</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>6.000000</td>\n      <td>0.127601</td>\n      <td>-0.166076</td>\n      <td>-0.301671</td>\n      <td>-0.059409</td>\n      <td>-0.513522</td>\n      <td>-0.257863</td>\n      <td>-0.085936</td>\n      <td>0.039904</td>\n      <td>0.061149</td>\n      <td>...</td>\n      <td>-0.177258</td>\n      <td>-0.200775</td>\n      <td>-0.119451</td>\n      <td>-0.166076</td>\n      <td>-0.166076</td>\n      <td>-0.177258</td>\n      <td>-0.115064</td>\n      <td>-0.166076</td>\n      <td>-0.128224</td>\n      <td>-0.142276</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.000000</td>\n      <td>0.241031</td>\n      <td>0.373866</td>\n      <td>0.366468</td>\n      <td>0.491108</td>\n      <td>0.558401</td>\n      <td>0.255930</td>\n      <td>0.590143</td>\n      <td>0.712210</td>\n      <td>0.758124</td>\n      <td>...</td>\n      <td>0.528136</td>\n      <td>0.477463</td>\n      <td>0.593772</td>\n      <td>0.451001</td>\n      <td>0.529090</td>\n      <td>0.600438</td>\n      <td>0.533803</td>\n      <td>0.528136</td>\n      <td>0.559923</td>\n      <td>0.516365</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.000000</td>\n      <td>0.996894</td>\n      <td>6.698910</td>\n      <td>7.533774</td>\n      <td>9.230570</td>\n      <td>12.685846</td>\n      <td>15.840967</td>\n      <td>14.562446</td>\n      <td>5.736815</td>\n      <td>14.767654</td>\n      <td>...</td>\n      <td>15.840967</td>\n      <td>15.840967</td>\n      <td>12.685846</td>\n      <td>9.230570</td>\n      <td>15.840967</td>\n      <td>14.767654</td>\n      <td>9.870119</td>\n      <td>9.870119</td>\n      <td>15.840967</td>\n      <td>14.767654</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 46 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "li = []\n",
    "for filename in file_list[:50]:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# df = pd.read_csv(file_list[0])\n",
    "df.shape\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          point.id motherset      origin  original.label  diff.score  \\\n0  wine_point_3594      wine  regression               7    0.050492   \n1  wine_point_5089      wine  regression               5    0.082237   \n2  wine_point_1912      wine  regression               6    0.290201   \n3  wine_point_4908      wine  regression               5    0.053559   \n4  wine_point_2246      wine  regression               7    0.420300   \n\n  ground.truth  fixed.acidity  volatile.acidity  citric.acid  residual.sugar  \\\n0      nominal      -1.245962         -0.362411    -0.265853       -0.261304   \n1      anomaly       0.759540          0.973867     0.215849       -0.534540   \n2      nominal      -0.088942         -0.969809    -0.403482       -0.870829   \n3      anomaly       0.219597          0.973867     0.284664        0.138039   \n4      nominal       0.219597         -0.180191    -0.541112        0.348220   \n\n   ...  noise..24  noise..25  noise..26  noise..27  noise..28  noise..29  \\\n0  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n1  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n2  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n3  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n4  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n\n   noise..30  noise..31  noise..32  noise..33  \n0        NaN        NaN        NaN        NaN  \n1        NaN        NaN        NaN        NaN  \n2        NaN        NaN        NaN        NaN  \n3        NaN        NaN        NaN        NaN  \n4        NaN        NaN        NaN        NaN  \n\n[5 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>point.id</th>\n      <th>motherset</th>\n      <th>origin</th>\n      <th>original.label</th>\n      <th>diff.score</th>\n      <th>ground.truth</th>\n      <th>fixed.acidity</th>\n      <th>volatile.acidity</th>\n      <th>citric.acid</th>\n      <th>residual.sugar</th>\n      <th>...</th>\n      <th>noise..24</th>\n      <th>noise..25</th>\n      <th>noise..26</th>\n      <th>noise..27</th>\n      <th>noise..28</th>\n      <th>noise..29</th>\n      <th>noise..30</th>\n      <th>noise..31</th>\n      <th>noise..32</th>\n      <th>noise..33</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wine_point_3594</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>7</td>\n      <td>0.050492</td>\n      <td>nominal</td>\n      <td>-1.245962</td>\n      <td>-0.362411</td>\n      <td>-0.265853</td>\n      <td>-0.261304</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wine_point_5089</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>5</td>\n      <td>0.082237</td>\n      <td>anomaly</td>\n      <td>0.759540</td>\n      <td>0.973867</td>\n      <td>0.215849</td>\n      <td>-0.534540</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wine_point_1912</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>6</td>\n      <td>0.290201</td>\n      <td>nominal</td>\n      <td>-0.088942</td>\n      <td>-0.969809</td>\n      <td>-0.403482</td>\n      <td>-0.870829</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wine_point_4908</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>5</td>\n      <td>0.053559</td>\n      <td>anomaly</td>\n      <td>0.219597</td>\n      <td>0.973867</td>\n      <td>0.284664</td>\n      <td>0.138039</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>wine_point_2246</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>7</td>\n      <td>0.420300</td>\n      <td>nominal</td>\n      <td>0.219597</td>\n      <td>-0.180191</td>\n      <td>-0.541112</td>\n      <td>0.348220</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 50 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"point.id\", \"motherset\", \"origin\", \"original.label\", \"diff.score\"])\n",
    "df = df[df.columns.drop(list(df.filter(regex='noise')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ground.truth            0\nfixed.acidity           0\nvolatile.acidity        0\ncitric.acid             0\nresidual.sugar          0\nchlorides               0\nfree.sulfur.dioxide     0\ntotal.sulfur.dioxide    0\ndensity                 0\npH                      0\nsulphates               0\nalcohol                 0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(185150, 12)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data = df.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "df[\"ground.truth\"] = enc.fit_transform(df[\"ground.truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = data[\"ground.truth\"]\n",
    "data_x = data.drop(columns = [\"ground.truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(148120, 11)"
     },
     "metadata": {},
     "execution_count": 12
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(148120,)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=1)\n",
    "X_train.to_numpy().shape\n",
    "y_train.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "\tdef __init__(self, run_config):\n",
    "\t\tself._training_dataset = None\n",
    "\t\tself._validation_dataset = None\n",
    "\t\tself._run_config = run_config\n",
    "\n",
    "\t\tself.load_datasets()\n",
    "\t\t\n",
    "\tdef load_datasets(self):\n",
    "\t\tself._training_dataset = CustomDataset(X_train, y_train)\n",
    "\t\tself._validation_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "\tdef get_data_loaders(self) -> Tuple[DataLoader]:\n",
    "\t\treturn (\n",
    "\t\t\tDataLoader(self._training_dataset, batch_size=self._run_config.batch_size, shuffle=True, num_workers=self._run_config.workers, pin_memory=True), \n",
    "\t\t\tDataLoader(self._validation_dataset, batch_size=self._run_config.batch_size, shuffle=True, num_workers=self._run_config.workers, pin_memory=True)\n",
    "\t\t)\n",
    "\n",
    "\tdef get_datasets(self) -> Tuple[Dataset]:\n",
    "\t\treturn self._training_dataset, self._validation_dataset\n",
    "\n",
    "\tdef get_datasets_sizes(self) -> Tuple[int]:\n",
    "\t\treturn len(self._training_dataset), len(self._validation_dataset)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self, X, Y):\n",
    "\t\tself.X = X.to_numpy()\n",
    "\t\tself.Y = Y.to_numpy()\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn torch.from_numpy(self.X[idx].astype('float32')) , self.Y[idx].astype('float32') \n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, in_size=11, out_size=1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_size, 256)\n",
    "        self.linear_2 = nn.Linear(256, 256)\n",
    "        self.linear_3 = nn.Linear(256, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.linear_1(x))\n",
    "        out = F.leaky_relu(self.linear_2(out))\n",
    "        out = F.sigmoid(self.linear_3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LinearModel(\n  (linear_1): Linear(in_features=11, out_features=256, bias=True)\n  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n  (linear_3): Linear(in_features=256, out_features=1, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 20
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0/50\n----------\nTraining step => Loss: 0.4876 Acc: 0.7643\nEvaluation step => Loss: 0.4701 Acc: 35.5673\nEpoch 1/50\n----------\nTraining step => Loss: 0.4516 Acc: 0.7874\nEvaluation step => Loss: 0.4476 Acc: 35.2890\nEpoch 2/50\n----------\nTraining step => Loss: 0.4307 Acc: 0.7996\nEvaluation step => Loss: 0.4271 Acc: 34.5660\nEpoch 3/50\n----------\nTraining step => Loss: 0.4122 Acc: 0.8107\nEvaluation step => Loss: 0.4150 Acc: 35.4809\nEpoch 4/50\n----------\nTraining step => Loss: 0.3955 Acc: 0.8198\nEvaluation step => Loss: 0.3942 Acc: 35.0784\nEpoch 5/50\n----------\nTraining step => Loss: 0.3794 Acc: 0.8301\nEvaluation step => Loss: 0.3800 Acc: 34.7682\nEpoch 6/50\n----------\nTraining step => Loss: 0.3640 Acc: 0.8398\nEvaluation step => Loss: 0.3652 Acc: 35.3807\nEpoch 7/50\n----------\nTraining step => Loss: 0.3491 Acc: 0.8487\nEvaluation step => Loss: 0.3529 Acc: 34.3298\nEpoch 8/50\n----------\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0d3687f79f78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;31m# statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "\tdevice = \"cuda\"\n",
    "\tnum_epochs = 50\n",
    "\tlr = 0.001\n",
    "\tbatch_size = 64\n",
    "\ttest_batch_size = 5\n",
    "\tworkers = 0\n",
    "\n",
    "data_handler = DataHandler(Config)\n",
    "train_dataset, validation_dataset = data_handler.get_datasets()\n",
    "train_loader, validation_loader = data_handler.get_data_loaders()\n",
    "training_dataset_size, validation_dataset_size = data_handler.get_datasets_sizes()\n",
    "\n",
    "best_model_wts = None\n",
    "best_acc = 0.0\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = LinearModel()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n",
    "\n",
    "loss_criterion = torch.nn.BCELoss()\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "for epoch in range(Config.num_epochs):\n",
    "\tprint('Epoch {}/{}'.format(epoch, Config.num_epochs))\n",
    "\tprint('-' * 10)\n",
    "\n",
    "\t########### Training step ###########\n",
    "\tmodel = model.train()\n",
    "\ttraining_loss = []\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\t\t\t\n",
    "\tfor i, data in enumerate(train_loader):\n",
    "\t\tx_batch, label_batch = data\n",
    "\t\tx_batch, label_batch = x_batch.to(device), label_batch.to(device)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = model(x_batch)\n",
    "\t\tpreds = torch.round(outputs)\n",
    "\n",
    "\t\tlabel_batch = label_batch.view((-1, 1))\n",
    "\t\tloss = loss_criterion(outputs, label_batch)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\t\t# statistics\n",
    "\t\trunning_loss += loss.item() * x_batch.size(0)\n",
    "\t\trunning_corrects += torch.sum(preds == label_batch.detach())\n",
    "\t\ttraining_loss.append(loss.item())\n",
    "\n",
    "\tepoch_loss = running_loss / training_dataset_size\n",
    "\tepoch_acc = running_corrects.double() / training_dataset_size\n",
    "\n",
    "\tprint('Training step => Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "\t\tepoch_loss, epoch_acc\n",
    "\t))\n",
    "\n",
    "\n",
    "\t########### Validation step ###########\n",
    "\tmodel = model.eval()\n",
    "\tvalidation_loss = []\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\n",
    "\tfor i, data in enumerate(validation_loader):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tx_batch, label_batch = data\n",
    "\t\t\tx_batch, label_batch = x_batch.to(device), label_batch.to(device)\n",
    "\n",
    "\t\t\toutputs = model(x_batch)\n",
    "\t\t\tpreds = torch.round(outputs)\n",
    "\n",
    "\t\t\tlabel_batch = label_batch.view((-1, 1))\n",
    "\t\t\tloss = loss_criterion(outputs, label_batch)\n",
    "\n",
    "\t\t\t# statistics\n",
    "\t\t\trunning_loss += loss.item() * x_batch.size(0)\n",
    "\t\t\trunning_corrects += torch.sum(preds == label_batch.detach())\n",
    "\t\t\tvalidation_loss.append(loss.item())\n",
    "\t\t\t\n",
    "\tepoch_loss = running_loss / validation_dataset_size\n",
    "\tepoch_acc = running_corrects.double() / validation_dataset_size\n",
    "\n",
    "\tprint('Evaluation step => Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "\t\tepoch_loss, epoch_acc\n",
    "\t))\n",
    "\n",
    "\t#Save the best model based on accuracy\n",
    "\tif epoch_acc > best_acc:\n",
    "\t\tbest_acc = epoch_acc\n",
    "\t\tbest_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\t#Checkpoint\n",
    "\ttorch.save({\n",
    "\t\t\"epoch\": epoch,\n",
    "\t\t\"model_state_dict\": model.state_dict(),\n",
    "\t\t\"optimizer_state_dict\": optimizer.state_dict()\n",
    "\t}, \"./checkpoints/ckp.pt\")\n",
    "\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "\ttime_elapsed // 60, time_elapsed % 60\n",
    "))\n",
    "print('Best (so far) validation Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "\n",
    "print('-' * 10)\n",
    "print('### Final results ###\\n')\n",
    "print('Best validation Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('torch': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596144356610"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}