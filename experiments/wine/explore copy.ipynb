{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"D:\\\\Storage\\\\AnomalyDetection\\\\wine\\\\benchmarks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of files: 1210\nExample path: D:\\Storage\\AnomalyDetection\\wine\\benchmarks\\wine_benchmark_0001.csv\n"
    }
   ],
   "source": [
    "file_list = list(pathlib.Path(dataset_path).glob('*.csv'))\n",
    "print(f\"Number of files: {len(file_list)}\")\n",
    "print(f\"Example path: {file_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(185150, 50)"
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       original.label     diff.score  fixed.acidity  volatile.acidity  \\\ncount   185150.000000  185150.000000  185150.000000     185150.000000   \nmean         5.821820       0.184060      -0.003067         -0.001873   \nstd          0.873348       0.179302       0.994429          0.994004   \nmin          3.000000       0.002164      -2.634386         -1.577208   \n25%          5.000000       0.061595      -0.628884         -0.666110   \n50%          6.000000       0.127601      -0.166076         -0.301671   \n75%          6.000000       0.241031       0.373866          0.366468   \nmax          9.000000       0.996894       6.698910          7.533774   \n\n         citric.acid  residual.sugar      chlorides  free.sulfur.dioxide  \\\ncount  185150.000000   185150.000000  185150.000000        185150.000000   \nmean       -0.004124        0.000977      -0.005601             0.000519   \nstd         0.998160        0.999860       0.975796             0.994600   \nmin        -2.192664       -1.017956      -1.342536            -1.663455   \n25%        -0.472297       -0.765739      -0.514759            -0.762016   \n50%        -0.059409       -0.513522      -0.257863            -0.085936   \n75%         0.491108        0.558401       0.255930             0.590143   \nmax         9.230570       12.685846      15.840967            14.562446   \n\n       total.sulfur.dioxide        density  ...     noise..24     noise..25  \\\ncount         185150.000000  185150.000000  ...  37030.000000  37030.000000   \nmean              -0.000257      -0.002645  ...     -0.002497     -0.001473   \nstd                0.998973       1.001061  ...      1.010434      0.997907   \nmin               -1.941631      -2.529997  ...     -2.975988     -2.789406   \n25%               -0.667787      -0.792562  ...     -0.702685     -0.649336   \n50%                0.039904       0.061149  ...     -0.177258     -0.200775   \n75%                0.712210       0.758124  ...      0.528136      0.477463   \nmax                5.736815      14.767654  ...     15.840967     15.840967   \n\n          noise..26     noise..27     noise..28     noise..29     noise..30  \\\ncount  37030.000000  37030.000000  37030.000000  37030.000000  37030.000000   \nmean       0.004486     -0.002218      0.004315     -0.006058     -0.006279   \nstd        0.998337      1.000018      1.016338      0.992243      1.007432   \nmin       -2.433288     -2.634386     -2.557251     -3.100376     -2.975988   \n25%       -0.680540     -0.628884     -0.706019     -0.737004     -0.674810   \n50%       -0.119451     -0.166076     -0.166076     -0.177258     -0.115064   \n75%        0.593772      0.451001      0.529090      0.600438      0.533803   \nmax       12.685846      9.230570     15.840967     14.767654      9.870119   \n\n          noise..31     noise..32     noise..33  \ncount  37030.000000  37030.000000  37030.000000  \nmean      -0.004379      0.000239     -0.009774  \nstd        1.003568      1.003339      0.993068  \nmin       -3.100376     -3.100376     -3.100376  \n25%       -0.706019     -0.609926     -0.680540  \n50%       -0.166076     -0.128224     -0.142276  \n75%        0.528136      0.559923      0.516365  \nmax        9.870119     15.840967     14.767654  \n\n[8 rows x 46 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original.label</th>\n      <th>diff.score</th>\n      <th>fixed.acidity</th>\n      <th>volatile.acidity</th>\n      <th>citric.acid</th>\n      <th>residual.sugar</th>\n      <th>chlorides</th>\n      <th>free.sulfur.dioxide</th>\n      <th>total.sulfur.dioxide</th>\n      <th>density</th>\n      <th>...</th>\n      <th>noise..24</th>\n      <th>noise..25</th>\n      <th>noise..26</th>\n      <th>noise..27</th>\n      <th>noise..28</th>\n      <th>noise..29</th>\n      <th>noise..30</th>\n      <th>noise..31</th>\n      <th>noise..32</th>\n      <th>noise..33</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>185150.000000</td>\n      <td>...</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n      <td>37030.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.821820</td>\n      <td>0.184060</td>\n      <td>-0.003067</td>\n      <td>-0.001873</td>\n      <td>-0.004124</td>\n      <td>0.000977</td>\n      <td>-0.005601</td>\n      <td>0.000519</td>\n      <td>-0.000257</td>\n      <td>-0.002645</td>\n      <td>...</td>\n      <td>-0.002497</td>\n      <td>-0.001473</td>\n      <td>0.004486</td>\n      <td>-0.002218</td>\n      <td>0.004315</td>\n      <td>-0.006058</td>\n      <td>-0.006279</td>\n      <td>-0.004379</td>\n      <td>0.000239</td>\n      <td>-0.009774</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.873348</td>\n      <td>0.179302</td>\n      <td>0.994429</td>\n      <td>0.994004</td>\n      <td>0.998160</td>\n      <td>0.999860</td>\n      <td>0.975796</td>\n      <td>0.994600</td>\n      <td>0.998973</td>\n      <td>1.001061</td>\n      <td>...</td>\n      <td>1.010434</td>\n      <td>0.997907</td>\n      <td>0.998337</td>\n      <td>1.000018</td>\n      <td>1.016338</td>\n      <td>0.992243</td>\n      <td>1.007432</td>\n      <td>1.003568</td>\n      <td>1.003339</td>\n      <td>0.993068</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.000000</td>\n      <td>0.002164</td>\n      <td>-2.634386</td>\n      <td>-1.577208</td>\n      <td>-2.192664</td>\n      <td>-1.017956</td>\n      <td>-1.342536</td>\n      <td>-1.663455</td>\n      <td>-1.941631</td>\n      <td>-2.529997</td>\n      <td>...</td>\n      <td>-2.975988</td>\n      <td>-2.789406</td>\n      <td>-2.433288</td>\n      <td>-2.634386</td>\n      <td>-2.557251</td>\n      <td>-3.100376</td>\n      <td>-2.975988</td>\n      <td>-3.100376</td>\n      <td>-3.100376</td>\n      <td>-3.100376</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.000000</td>\n      <td>0.061595</td>\n      <td>-0.628884</td>\n      <td>-0.666110</td>\n      <td>-0.472297</td>\n      <td>-0.765739</td>\n      <td>-0.514759</td>\n      <td>-0.762016</td>\n      <td>-0.667787</td>\n      <td>-0.792562</td>\n      <td>...</td>\n      <td>-0.702685</td>\n      <td>-0.649336</td>\n      <td>-0.680540</td>\n      <td>-0.628884</td>\n      <td>-0.706019</td>\n      <td>-0.737004</td>\n      <td>-0.674810</td>\n      <td>-0.706019</td>\n      <td>-0.609926</td>\n      <td>-0.680540</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>6.000000</td>\n      <td>0.127601</td>\n      <td>-0.166076</td>\n      <td>-0.301671</td>\n      <td>-0.059409</td>\n      <td>-0.513522</td>\n      <td>-0.257863</td>\n      <td>-0.085936</td>\n      <td>0.039904</td>\n      <td>0.061149</td>\n      <td>...</td>\n      <td>-0.177258</td>\n      <td>-0.200775</td>\n      <td>-0.119451</td>\n      <td>-0.166076</td>\n      <td>-0.166076</td>\n      <td>-0.177258</td>\n      <td>-0.115064</td>\n      <td>-0.166076</td>\n      <td>-0.128224</td>\n      <td>-0.142276</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.000000</td>\n      <td>0.241031</td>\n      <td>0.373866</td>\n      <td>0.366468</td>\n      <td>0.491108</td>\n      <td>0.558401</td>\n      <td>0.255930</td>\n      <td>0.590143</td>\n      <td>0.712210</td>\n      <td>0.758124</td>\n      <td>...</td>\n      <td>0.528136</td>\n      <td>0.477463</td>\n      <td>0.593772</td>\n      <td>0.451001</td>\n      <td>0.529090</td>\n      <td>0.600438</td>\n      <td>0.533803</td>\n      <td>0.528136</td>\n      <td>0.559923</td>\n      <td>0.516365</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.000000</td>\n      <td>0.996894</td>\n      <td>6.698910</td>\n      <td>7.533774</td>\n      <td>9.230570</td>\n      <td>12.685846</td>\n      <td>15.840967</td>\n      <td>14.562446</td>\n      <td>5.736815</td>\n      <td>14.767654</td>\n      <td>...</td>\n      <td>15.840967</td>\n      <td>15.840967</td>\n      <td>12.685846</td>\n      <td>9.230570</td>\n      <td>15.840967</td>\n      <td>14.767654</td>\n      <td>9.870119</td>\n      <td>9.870119</td>\n      <td>15.840967</td>\n      <td>14.767654</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 46 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "li = []\n",
    "for filename in file_list[:50]:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# df = pd.read_csv(file_list[0])\n",
    "df.shape\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          point.id motherset      origin  original.label  diff.score  \\\n0  wine_point_3594      wine  regression               7    0.050492   \n1  wine_point_5089      wine  regression               5    0.082237   \n2  wine_point_1912      wine  regression               6    0.290201   \n3  wine_point_4908      wine  regression               5    0.053559   \n4  wine_point_2246      wine  regression               7    0.420300   \n\n  ground.truth  fixed.acidity  volatile.acidity  citric.acid  residual.sugar  \\\n0      nominal      -1.245962         -0.362411    -0.265853       -0.261304   \n1      anomaly       0.759540          0.973867     0.215849       -0.534540   \n2      nominal      -0.088942         -0.969809    -0.403482       -0.870829   \n3      anomaly       0.219597          0.973867     0.284664        0.138039   \n4      nominal       0.219597         -0.180191    -0.541112        0.348220   \n\n   ...  noise..24  noise..25  noise..26  noise..27  noise..28  noise..29  \\\n0  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n1  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n2  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n3  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n4  ...        NaN        NaN        NaN        NaN        NaN        NaN   \n\n   noise..30  noise..31  noise..32  noise..33  \n0        NaN        NaN        NaN        NaN  \n1        NaN        NaN        NaN        NaN  \n2        NaN        NaN        NaN        NaN  \n3        NaN        NaN        NaN        NaN  \n4        NaN        NaN        NaN        NaN  \n\n[5 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>point.id</th>\n      <th>motherset</th>\n      <th>origin</th>\n      <th>original.label</th>\n      <th>diff.score</th>\n      <th>ground.truth</th>\n      <th>fixed.acidity</th>\n      <th>volatile.acidity</th>\n      <th>citric.acid</th>\n      <th>residual.sugar</th>\n      <th>...</th>\n      <th>noise..24</th>\n      <th>noise..25</th>\n      <th>noise..26</th>\n      <th>noise..27</th>\n      <th>noise..28</th>\n      <th>noise..29</th>\n      <th>noise..30</th>\n      <th>noise..31</th>\n      <th>noise..32</th>\n      <th>noise..33</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wine_point_3594</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>7</td>\n      <td>0.050492</td>\n      <td>nominal</td>\n      <td>-1.245962</td>\n      <td>-0.362411</td>\n      <td>-0.265853</td>\n      <td>-0.261304</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wine_point_5089</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>5</td>\n      <td>0.082237</td>\n      <td>anomaly</td>\n      <td>0.759540</td>\n      <td>0.973867</td>\n      <td>0.215849</td>\n      <td>-0.534540</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wine_point_1912</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>6</td>\n      <td>0.290201</td>\n      <td>nominal</td>\n      <td>-0.088942</td>\n      <td>-0.969809</td>\n      <td>-0.403482</td>\n      <td>-0.870829</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wine_point_4908</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>5</td>\n      <td>0.053559</td>\n      <td>anomaly</td>\n      <td>0.219597</td>\n      <td>0.973867</td>\n      <td>0.284664</td>\n      <td>0.138039</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>wine_point_2246</td>\n      <td>wine</td>\n      <td>regression</td>\n      <td>7</td>\n      <td>0.420300</td>\n      <td>nominal</td>\n      <td>0.219597</td>\n      <td>-0.180191</td>\n      <td>-0.541112</td>\n      <td>0.348220</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 50 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"point.id\", \"motherset\", \"origin\", \"original.label\", \"diff.score\"])\n",
    "df = df[df.columns.drop(list(df.filter(regex='noise')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ground.truth            0\nfixed.acidity           0\nvolatile.acidity        0\ncitric.acid             0\nresidual.sugar          0\nchlorides               0\nfree.sulfur.dioxide     0\ntotal.sulfur.dioxide    0\ndensity                 0\npH                      0\nsulphates               0\nalcohol                 0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(185150, 12)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data = df.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "df[\"ground.truth\"] = enc.fit_transform(df[\"ground.truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = data[\"ground.truth\"]\n",
    "data_x = data.drop(columns = [\"ground.truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from typing import Dict, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "\tdef __init__(self, run_config):\n",
    "\t\tself._training_dataset = None\n",
    "\t\tself._validation_dataset = None\n",
    "\t\tself._run_config = run_config\n",
    "\n",
    "\t\tself.load_datasets()\n",
    "\t\t\n",
    "\tdef load_datasets(self):\n",
    "\t\tself._training_dataset = CustomDataset(X_train, y_train)\n",
    "\t\tself._validation_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "\tdef get_data_loaders(self) -> Tuple[DataLoader]:\n",
    "\t\treturn (\n",
    "\t\t\tDataLoader(self._training_dataset, batch_size=self._run_config.batch_size, shuffle=True, num_workers=self._run_config.workers, pin_memory=True), \n",
    "\t\t\tDataLoader(self._validation_dataset, batch_size=self._run_config.batch_size, shuffle=True, num_workers=self._run_config.workers, pin_memory=True)\n",
    "\t\t)\n",
    "\n",
    "\tdef get_datasets(self) -> Tuple[Dataset]:\n",
    "\t\treturn self._training_dataset, self._validation_dataset\n",
    "\n",
    "\tdef get_datasets_sizes(self) -> Tuple[int]:\n",
    "\t\treturn len(self._training_dataset), len(self._validation_dataset)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self, X, Y):\n",
    "\t\tself.X = X.to_numpy()\n",
    "\t\tself.Y = Y.to_numpy()\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.X[idx], self.Y[idx]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn size(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, in_size=11, out_size=1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_size, 256)\n",
    "        self.linear_2 = nn.Linear(256, 256)\n",
    "        self.linear_3 = nn.Linear(256, out_size)\n",
    "\n",
    "    def forward(x):\n",
    "        out = F.leaky_relu(self.linear_1(x))\n",
    "        out = F.leaky_relu(self.linear_1(out))\n",
    "        out = F.sigmoid(self.linear_1(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LinearModel(\n  (linear_1): Linear(in_features=11, out_features=256, bias=True)\n  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n  (linear_3): Linear(in_features=256, out_features=1, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 22
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0cd010bfc854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-08\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = LinearModel()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('torch': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596056429174"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}